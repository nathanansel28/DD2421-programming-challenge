{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "from data import *\n",
    "from model_xgb import hyperparam_tuning\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters found: {'subsample': 0.6, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.8}\n",
      "Best accuracy score: 0.6933999999999999\n"
     ]
    }
   ],
   "source": [
    "tuned_results = hyperparam_tuning(X_train, y_train)\n",
    "\n",
    "with open(\"model_xgb_params.json\", \"w\") as outfile: \n",
    "    json.dump(tuned_results.best_params_, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
      "Best parameters found: {'subsample': 0.6, 'n_estimators': 300, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 1.0}\n",
      "Best accuracy score: 0.6932\n"
     ]
    }
   ],
   "source": [
    "tuned_results = hyperparam_tuning(X_train, y_train, n_iter=1000)\n",
    "\n",
    "with open(\"model_xgb_params.json\", \"w\") as outfile: \n",
    "    json.dump(tuned_results.best_params_, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# The function that performs hyperparameter tuning and feature selection\n",
    "def hyperparam_tuning_with_feature_selection(X_train, y_train, k=5, n_iter=20) -> RandomizedSearchCV:\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Separate categorical and numerical columns\n",
    "    cat_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    num_cols = X_train.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "    # Create a preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', num_cols),  # Keep numeric columns as they are\n",
    "            ('cat', OneHotEncoder(), cat_cols)  # One-hot encode categorical columns\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # XGBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y_train)), \n",
    "                                  eval_metric=\"mlogloss\", random_state=42)\n",
    "    \n",
    "    # Feature selection (recursive feature elimination with cross-validation)\n",
    "    feature_selector = RFECV(estimator=xgb_model, step=1, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Build a pipeline with preprocessing, feature selection, and model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('classifier', xgb_model)\n",
    "    ])\n",
    "    \n",
    "    # Hyperparameter grid for RandomizedSearchCV\n",
    "    param_dist = {\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'classifier__max_depth': [3, 6, 9, 12],\n",
    "        'classifier__n_estimators': [100, 300, 500],\n",
    "        'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "        'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'classifier__gamma': [0, 1, 5],\n",
    "        'classifier__min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "    \n",
    "    # Randomized search for hyperparameters\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline, param_distributions=param_dist, n_iter=n_iter,\n",
    "        scoring='accuracy', cv=cv, verbose=2, n_jobs=-1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit the random search\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Output best results\n",
    "    print(\"Best parameters found:\", random_search.best_params_)\n",
    "    print(\"Best accuracy score:\", random_search.best_score_)\n",
    "    \n",
    "    return random_search, feature_selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters found: {'classifier__subsample': 0.6, 'classifier__n_estimators': 100, 'classifier__min_child_weight': 3, 'classifier__max_depth': 6, 'classifier__learning_rate': 0.05, 'classifier__gamma': 1, 'classifier__colsample_bytree': 1.0}\n",
      "Best accuracy score: 0.6908\n"
     ]
    }
   ],
   "source": [
    "# Call the hyperparameter tuning function with feature selection\n",
    "random_search, feature_selector = hyperparam_tuning_with_feature_selection(X_train, y_train, k=5, n_iter=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

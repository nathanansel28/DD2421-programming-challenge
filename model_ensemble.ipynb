{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "from data import *\n",
    "from ensemble import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_weights(final_results, n_top=5):\n",
    "    # Sort by mean_accuracy (top 5 accuracies)\n",
    "    top_5_accuracies = sorted(final_results, key=lambda x: x['mean_accuracy'], reverse=True)[:n_top]\n",
    "\n",
    "    # Sort by mean_log_loss (bottom 5 log-losses)\n",
    "    bottom_5_loglosses = sorted(final_results, key=lambda x: x['mean_log_loss'])[:n_top]\n",
    "\n",
    "    print(f\"Top {n_top} Accuracies:\")\n",
    "    for i, result in enumerate(top_5_accuracies, 1):\n",
    "        print(f\"{i}. Weights: {[round(float(w), 2) for w in result['weights']]} | \"\n",
    "              f\"Mean Accuracy: {result['mean_accuracy']:.3f} | \"\n",
    "              f\"Std Accuracy: {result['std_accuracy']:.3f} | \"\n",
    "              f\"Mean Log Loss: {result['mean_log_loss']:.3f} | \"\n",
    "              f\"Std Log Loss: {result['std_log_loss']:.3f}\")\n",
    "\n",
    "    print(f\"\\nBottom {n_top} Log Losses:\")\n",
    "    for i, result in enumerate(bottom_5_loglosses, 1):\n",
    "        print(f\"{i}. Weights: {[round(float(w), 2) for w in result['weights']]} | \"\n",
    "              f\"Mean Accuracy: {result['mean_accuracy']:.3f} | \"\n",
    "              f\"Std Accuracy: {result['std_accuracy']:.3f} | \"\n",
    "              f\"Mean Log Loss: {result['mean_log_loss']:.3f} | \"\n",
    "              f\"Std Log Loss: {result['std_log_loss']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Top 10 Accuracies:\n",
      "1. Weights: [0.45, 0.1, 0.45] | Mean Accuracy: 0.697 | Std Accuracy: 0.014 | Mean Log Loss: 0.694 | Std Log Loss: 0.012\n",
      "2. Weights: [0.55, 0.15, 0.3] | Mean Accuracy: 0.697 | Std Accuracy: 0.016 | Mean Log Loss: 0.694 | Std Log Loss: 0.011\n",
      "3. Weights: [0.5, 0.1, 0.4] | Mean Accuracy: 0.696 | Std Accuracy: 0.016 | Mean Log Loss: 0.694 | Std Log Loss: 0.012\n",
      "4. Weights: [0.5, 0.15, 0.35] | Mean Accuracy: 0.696 | Std Accuracy: 0.016 | Mean Log Loss: 0.695 | Std Log Loss: 0.012\n",
      "5. Weights: [0.55, 0.25, 0.2] | Mean Accuracy: 0.696 | Std Accuracy: 0.012 | Mean Log Loss: 0.697 | Std Log Loss: 0.011\n",
      "6. Weights: [0.5, 0.05, 0.45] | Mean Accuracy: 0.696 | Std Accuracy: 0.015 | Mean Log Loss: 0.693 | Std Log Loss: 0.012\n",
      "7. Weights: [0.6, 0.1, 0.3] | Mean Accuracy: 0.696 | Std Accuracy: 0.017 | Mean Log Loss: 0.693 | Std Log Loss: 0.011\n",
      "8. Weights: [0.45, 0.15, 0.4] | Mean Accuracy: 0.696 | Std Accuracy: 0.014 | Mean Log Loss: 0.695 | Std Log Loss: 0.012\n",
      "9. Weights: [0.65, 0.1, 0.25] | Mean Accuracy: 0.696 | Std Accuracy: 0.015 | Mean Log Loss: 0.692 | Std Log Loss: 0.011\n",
      "10. Weights: [0.65, 0.0, 0.35] | Mean Accuracy: 0.696 | Std Accuracy: 0.013 | Mean Log Loss: 0.691 | Std Log Loss: 0.011\n",
      "\n",
      "Bottom 10 Log Losses:\n",
      "1. Weights: [0.75, 0.0, 0.25] | Mean Accuracy: 0.694 | Std Accuracy: 0.012 | Mean Log Loss: 0.690 | Std Log Loss: 0.010\n",
      "2. Weights: [0.7, 0.0, 0.3] | Mean Accuracy: 0.694 | Std Accuracy: 0.013 | Mean Log Loss: 0.691 | Std Log Loss: 0.011\n",
      "3. Weights: [0.8, 0.0, 0.2] | Mean Accuracy: 0.695 | Std Accuracy: 0.013 | Mean Log Loss: 0.691 | Std Log Loss: 0.010\n",
      "4. Weights: [0.65, 0.0, 0.35] | Mean Accuracy: 0.696 | Std Accuracy: 0.013 | Mean Log Loss: 0.691 | Std Log Loss: 0.011\n",
      "5. Weights: [0.85, 0.0, 0.15] | Mean Accuracy: 0.695 | Std Accuracy: 0.011 | Mean Log Loss: 0.691 | Std Log Loss: 0.010\n",
      "6. Weights: [0.6, 0.0, 0.4] | Mean Accuracy: 0.694 | Std Accuracy: 0.014 | Mean Log Loss: 0.691 | Std Log Loss: 0.012\n",
      "7. Weights: [0.7, 0.05, 0.25] | Mean Accuracy: 0.694 | Std Accuracy: 0.012 | Mean Log Loss: 0.691 | Std Log Loss: 0.011\n",
      "8. Weights: [0.75, 0.05, 0.2] | Mean Accuracy: 0.693 | Std Accuracy: 0.012 | Mean Log Loss: 0.691 | Std Log Loss: 0.010\n",
      "9. Weights: [0.65, 0.05, 0.3] | Mean Accuracy: 0.694 | Std Accuracy: 0.014 | Mean Log Loss: 0.691 | Std Log Loss: 0.011\n",
      "10. Weights: [0.9, 0.0, 0.1] | Mean Accuracy: 0.694 | Std Accuracy: 0.010 | Mean Log Loss: 0.691 | Std Log Loss: 0.009\n"
     ]
    }
   ],
   "source": [
    "with open('model_xgb_params.json', 'r') as file:\n",
    "    xgb_params = json.load(file)\n",
    "\n",
    "results, best_result = train_ensemble(X_train, y_train, mean_type='arithmetic', xgb_params=xgb_params)\n",
    "show_top_weights(results, n_top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results, best_result = \u001b[43mtrain_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgeometric\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxgb_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m show_top_weights(results, n_top=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\ensemble.py:143\u001b[39m, in \u001b[36mtrain_ensemble\u001b[39m\u001b[34m(X, y, xgb_params, mean_type, k, verbose)\u001b[39m\n\u001b[32m    140\u001b[39m gnb_model = Model(model_type=\u001b[33m'\u001b[39m\u001b[33mgnb\u001b[39m\u001b[33m'\u001b[39m, selected_features=[\u001b[33m'\u001b[39m\u001b[33mx2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx4\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx6\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx8\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx9\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx10\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx11\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    141\u001b[39m nn_model = Model(model_type=\u001b[33m'\u001b[39m\u001b[33mnn\u001b[39m\u001b[33m'\u001b[39m, nn_params=X_train_scaled.shape[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m gnb_model.fit(X_train, y_train)\n\u001b[32m    145\u001b[39m nn_model.fit(X_train_scaled, y_train, X_val=X_val_scaled, y_val=y_val, verbose=verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\base_models.py:36\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val, verbose)\u001b[39m\n\u001b[32m     33\u001b[39m X_train = \u001b[38;5;28mself\u001b[39m._select_features(X_train)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mgnb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxgb\u001b[39m\u001b[33m'\u001b[39m]: \n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_type == \u001b[33m'\u001b[39m\u001b[33mnn\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.fit(\n\u001b[32m     39\u001b[39m         X_train, y_train, \n\u001b[32m     40\u001b[39m         epochs=\u001b[32m20\u001b[39m, batch_size=\u001b[32m16\u001b[39m, verbose=verbose, \n\u001b[32m     41\u001b[39m         validation_data=(X_val, y_val)\n\u001b[32m     42\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\xgboost\\core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\xgboost\\sklearn.py:1599\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1579\u001b[39m model, metric, params = \u001b[38;5;28mself\u001b[39m._configure_fit(xgb_model, params)\n\u001b[32m   1580\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1581\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1582\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     feature_types=\u001b[38;5;28mself\u001b[39m.feature_types,\n\u001b[32m   1597\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1599\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1603\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1604\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1605\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1606\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1611\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1614\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\xgboost\\core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\xgboost\\training.py:181\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2100\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2101\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2104\u001b[39m     )\n\u001b[32m   2105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2106\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results, best_result = train_ensemble(X_train, y_train, mean_type='geometric', xgb_params=xgb_params)\n",
    "show_top_weights(results, n_top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights and performance metrics\n",
    "weights_xgb = [res['weights'][0] for res in results]\n",
    "weights_gnb = [res['weights'][1] for res in results]\n",
    "weights_nn = [res['weights'][2] for res in results]\n",
    "log_losses = [res['log_loss'] for res in results]\n",
    "accuracies = [res['accuracy'] for res in results]\n",
    "\n",
    "# Plot Log Loss vs. Model Weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(weights_xgb, log_losses, label='XGB Weight', alpha=0.6, marker='o', color='blue')\n",
    "plt.scatter(weights_gnb, log_losses, label='GNB Weight', alpha=0.6, marker='s', color='green')\n",
    "plt.scatter(weights_nn, log_losses, label='NN Weight', alpha=0.6, marker='^', color='red')\n",
    "plt.xlabel(\"Model Weights\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Ensemble Weights vs Log Loss\")\n",
    "\n",
    "# Plot Accuracy vs. Model Weights\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(weights_xgb, accuracies, label='XGB Weight', alpha=0.6, marker='o', color='blue')\n",
    "plt.scatter(weights_gnb, accuracies, label='GNB Weight', alpha=0.6, marker='s', color='green')\n",
    "plt.scatter(weights_nn, accuracies, label='NN Weight', alpha=0.6, marker='^', color='red')\n",
    "plt.xlabel(\"Model Weights\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Ensemble Weights vs Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(results, key=lambda x: x['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# Extract weights and performance metrics\n",
    "weights_xgb = [res['weights'][0] for res in results]\n",
    "weights_gnb = [res['weights'][1] for res in results]\n",
    "log_losses = [res['log_loss'] for res in results]\n",
    "accuracies = [res['accuracy'] for res in results]\n",
    "\n",
    "# Create grid for plotting\n",
    "weight_options = np.linspace(0, 1, 21)\n",
    "X, Y = np.meshgrid(weight_options, weight_options)\n",
    "Z_accuracy = np.full(X.shape, np.nan)\n",
    "Z_log_loss = np.full(X.shape, np.nan)\n",
    "\n",
    "# Fill the grid with accuracy and log loss values\n",
    "for i, w_xgb in enumerate(weight_options):\n",
    "    for j, w_gnb in enumerate(weight_options):\n",
    "        if w_xgb + w_gnb <= 1:\n",
    "            w_nn = 1 - (w_xgb + w_gnb)\n",
    "            idx = next(k for k, res in enumerate(results) if np.isclose(res['weights'][0], w_xgb) and np.isclose(res['weights'][1], w_gnb))\n",
    "            Z_accuracy[i, j] = accuracies[idx]\n",
    "            Z_log_loss[i, j] = log_losses[idx]\n",
    "\n",
    "# Plotting accuracy\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot Accuracy surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z_accuracy, cmap='viridis')\n",
    "ax1.set_xlabel('XGBoost Weight')\n",
    "ax1.set_ylabel('GNB Weight')\n",
    "ax1.set_zlabel('Accuracy')\n",
    "ax1.set_title('Accuracy vs Model Weights')\n",
    "\n",
    "# Plot Log Loss surface\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.plot_surface(X, Y, Z_log_loss, cmap='plasma')\n",
    "ax2.set_xlabel('XGBoost Weight')\n",
    "ax2.set_ylabel('GNB Weight')\n",
    "ax2.set_zlabel('Log Loss')\n",
    "ax2.set_title('Log Loss vs Model Weights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy: 0.6500 Â± 0.0087\n",
      "Submission file saved as submission_gnb.csv ðŸš€\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_35404\\2200029785.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[bool_cols] = X_train[bool_cols].astype(int)\n",
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_35404\\2200029785.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[col] = le.fit_transform(X_train[col])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize Gaussian NaÃ¯ve Bayes\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# K-Fold Cross Validation (Stratified)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_scores = []\n",
    "for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Train NaÃ¯ve Bayes\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate model\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    acc = accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_scores.append(acc)\n",
    "\n",
    "print(f\"Cross-validation accuracy: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "zfinal_model = GaussianNB()\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
    "\n",
    "# Save for Kaggle submission\n",
    "submission = pd.DataFrame({\"id\": test_raw.index, \"y\": y_test_pred_labels})\n",
    "submission.to_csv(\"submission_gnb.csv\", index=False)\n",
    "\n",
    "print(\"Submission file saved as submission_gnb.csv ðŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature     F-Score        P-Value\n",
      "9      x11  643.360054  2.401895e-249\n",
      "3       x4  641.254523  1.282248e-248\n",
      "8      x10    5.667893   3.477389e-03\n",
      "2       x3    3.787020   2.272811e-02\n",
      "4       x5    1.624479   1.971183e-01\n",
      "0       x1    1.042293   3.527219e-01\n",
      "1       x2    0.779779   4.585630e-01\n",
      "6       x8    0.655626   5.191617e-01\n",
      "5       x6    0.475849   6.213858e-01\n",
      "7       x9    0.376437   6.863216e-01\n",
      "10     x13    0.170474   8.432700e-01\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "when `importance_getter=='auto'`, the underlying estimator GaussianNB should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m gnb = GaussianNB()\n\u001b[32m     22\u001b[39m rfe = RFE(gnb, n_features_to_select=\u001b[32m8\u001b[39m)  \u001b[38;5;66;03m# Adjust the number of features you want\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mrfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Get selected features\u001b[39;00m\n\u001b[32m     26\u001b[39m selected_features = X_num.columns[rfe.support_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:276\u001b[39m, in \u001b[36mRFE.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    274\u001b[39m     routed_params = Bunch(estimator=Bunch(fit=fit_params))\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:335\u001b[39m, in \u001b[36mRFE._fit\u001b[39m\u001b[34m(self, X, y, step_score, **fit_params)\u001b[39m\n\u001b[32m    332\u001b[39m estimator.fit(X[:, features], y, **fit_params)\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m importances = \u001b[43m_get_feature_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimportance_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msquare\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m ranks = np.argsort(importances)\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# for sparse case ranks is matrix\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\SUTD\\KTH\\DD2421 Machine Learning\\DD2421-programming-challenge\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:234\u001b[39m, in \u001b[36m_get_feature_importances\u001b[39m\u001b[34m(estimator, getter, transform_func, norm_order)\u001b[39m\n\u001b[32m    232\u001b[39m         getter = attrgetter(\u001b[33m\"\u001b[39m\u001b[33mfeature_importances_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    235\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen `importance_getter==\u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m`, the underlying \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mestimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m should have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`coef_` or `feature_importances_` attribute. Either \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    238\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpass a fitted estimator to feature selector or call fit \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    239\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbefore calling transform.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    240\u001b[39m         )\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    242\u001b[39m     getter = attrgetter(getter)\n",
      "\u001b[31mValueError\u001b[39m: when `importance_getter=='auto'`, the underlying estimator GaussianNB should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Compute ANOVA F-score for numerical features\n",
    "numerical_features = train_raw.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_num = train_raw[numerical_features]\n",
    "y = train_raw['y']\n",
    "\n",
    "anova_f, anova_p = f_classif(X_num, y)\n",
    "feature_importance = pd.DataFrame({'Feature': numerical_features, 'F-Score': anova_f, 'P-Value': anova_p})\n",
    "feature_importance = feature_importance.sort_values(by=\"F-Score\", ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Fit GNB model\n",
    "gnb = GaussianNB()\n",
    "rfe = RFE(gnb, n_features_to_select=8)  # Adjust the number of features you want\n",
    "rfe.fit(X_num, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X_num.columns[rfe.support_]\n",
    "print(f\"Selected Features: {selected_features.tolist()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
